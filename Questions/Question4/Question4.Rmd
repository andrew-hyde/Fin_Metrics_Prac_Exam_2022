---
title: 'Question 4: Volatility Comparison'
author: "Andrew Hyde"
date: '2022-11-26'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf.
# These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Question 4: Volatility Comparison
# load functions
library(tidyverse)
list.files('code/', full.names = T, recursive = T) %>% .[grepl('.R', .)] %>% as.list() %>% walk(~source(.))

# Lets load in example data, and see how this can be stored and later called from your 'data' folder.
library(tidyverse)
# import the data
T40 <- read_rds("data/T40.rds")
T40$Tickers <- gsub("SJ|Equity", "", T40$Tickers)
usdzar <- read_rds("data/usdzar.rds")

```

# Introduction

In this section I run a Principal Component Analysis to understand the concentration and commonality of returns within the
Top 40 indexes.

# Data

I begin testing the data for missing values (NA) and then plotting the data to see if there are any significant outliers.

```{r message=FALSE, warning=FALSE}

graph_q4.1_func(df_data = q4.1_data,
                title = "Returns of ALSI Weighted Index",
                subtitle = "From 2008 to 2022",
                caption = "ALSI (J200) 40 Indexes",
                xlabel = "Date",
                ylabel = "Returns")


```

Now I winsorize the data by setting any return above 0.25% equal to 0.25% and create a weighted return column based on the J200 weights. I also determine if there are missing values that will effect the results of my analysis and then the data is mean-centered.

The index weight column J200 and Index_Name column both have missing values, however this is nothing to be concerned about as this just implies that some stocks don't have weights as they were included in the Top 40 or will be in the future but are not included at present. Therefore, I remove missing values from the weights column.

```{r message=FALSE, warning=FALSE}

# determine if there are NA's in the data
missing_values_by_column <- colSums(is.na(T40))
T40["J200"][is.na(T40["J200"])] <- 0 

T40 <- T40 %>% 
    select(-J400) %>%
    # can also winsorize the data
    mutate(Return = ifelse(Return > 0.25, 0.25, ifelse(Return < -0.25, -0.25, Return))) %>%
    arrange(date) %>%
    group_by(Tickers) %>%
    mutate(Return = Return - mean(Return)) %>%
    mutate(weighted_return = J200*Return) %>% 
    ungroup() %>% 
    filter(date > first(date))
    #select(date, Tickers, weighted_return)


# Plot the clean data
graph_q4.2_func(df_data = q4.2_data,
                title = "Returns of ALSI Weighted Indexes",
                subtitle = "From 2008 to 2022",
                caption = "ALSI (J200) 40 Indexes",
                xlabel = "Date",
                ylabel = "Returns")



```

# PCA

Now that the data has been cleaned I begin the principal component analysis.

I make use of the FactoMineR package to run my first PCA, however once I made the data wide missing values for the returns of certain equities in the index at specific dates. This may be because of listing and de-listing on a public exchange. I now use the impute function to replace missing values with using "Drawn_Distribution_Own" method, however this did not works and then moved on to replacing the missing values with the "Drawn_Distribution_Collective" method.

```{r}
library(FactoMineR)
library(factoextra)

# pca requires wide, numeric data: REMORE date for PCA
# data_T40_wide <- T40 %>% filter(date > lubridate::ymd(20080101)) %>% 
#     select(date, Tickers, weighted_return) %>% 
#     mutate(Return = weighted_return) %>% # rename for the impute_missing_value function
#         spread(Tickers, Return) %>% select(-date) 

# test for NA's since PCA func returned NA error

return_mat_Quick = T40 %>%  
    mutate(Return = weighted_return) %>% 
    select(date, Tickers, Return) %>% 
    filter(date > lubridate::ymd(20080101)) %>% 
    spread(Tickers, Return)

# need date column for impute_missing_returns
pca_T40_data <- impute_missing_returns(return_mat = return_mat_Quick, 
                                       impute_returns_method = "Drawn_Distribution_Collective")
# pca requires wide, numeric data: REMORE date for PCA
pca_T40 <- PCA(pca_T40_data %>% select(-date), graph = FALSE)


factoextra::fviz_screeplot(pca_T40, ncp = 10)
# Contributions of variables on PC1

```

From the scree plot above one can see the first 10 or 10 largest principal components. Each component (1-10) display's it's contribution to explaining the variation in the Top 40 data set. The first component explains approximately 8.5% of the variation. Components 2 through 10, each explain roughly between 2.6% and 2% of the variation.

```{r}
factoextra::fviz_contrib(pca_T40, choice = "var", axes = 1, top = 40, fill = "darkorange")
```

The graph above provides the contribution each stop makes to the first component. The largest contributors to the are financials such as Standard Bank, First Rand, Invested, Nedbank, ABSA and Sanlam. Given the commonality of sector among the largest contributors to the first principal component. It is likely that the first principal competent is interest rates, given financial equities relationship to interest rates i.e. changing interest rates would significantly change financials share prices


```{r}

fviz_pca_var(pca_T40, col.var = "contrib", repel = T) + theme_minimal()

```

# calc rolling contributions with PerformanceAnalytics
or 
starification from prac 2 bonus


```{r}

library(tidyverse)
library(rmsfuns)
pacman::p_load("tidyr", "tbl2xts","devtools","lubridate", "readr", "PerformanceAnalytics", "ggplot2", "dplyr")

# ALSI (J200) weighted returns
T40_index <- T40 %>% arrange(date) %>% 
    group_by(Tickers) %>% 
    mutate(Return = Return * J200) %>% 
    select(date, Tickers, Return) %>% 
    filter(!is.na(Return)) %>% 
    mutate(YearMonth = format(date, "%Y%B")) %>% 
    ungroup()


Idx_Cons <- T40_index %>% 
            group_by(Tickers) %>% 
            filter(date == first(date)) %>% 
            ungroup() %>% 
            pull(Tickers) %>% unique

Index_data <- T40_index %>% 
                filter(Tickers %in% Idx_Cons) %>% 
                filter(date > ymd(20080101))

# Control for outliers by winzorising the data

T40_Index_data <- Index_data %>%
    group_by(Tickers) %>% 
    mutate(Top = quantile(Return, 0.99), Bot = quantile(Return, 0.01)) %>% 
    mutate(Return = ifelse(Return > Top, Top, ifelse(Return < Bot, Bot, Return))) %>%
    ungroup()

# Now use another asset such as currency that quickly reflects volatility 
usdzar_data <- usdzar %>% 
     # this will produce NA at 1st date
    mutate(Return = Price - lag(Price)) %>%
    # therefore calc returns before you filter for date
               filter(date > ymd(20080101)) %>% 
               select(-Name)

# calc monthly SD from daily data
usdzarSD <- usdzar_data %>% 
    mutate(YearMonth = format(date, "%Y%B")) %>% 
    group_by(YearMonth) %>% summarise(SD = sd(Return)*sqrt(252)) %>% 
    # Top Decile Quantile overall (highly volatile month for ZAR:
    mutate(TopQtile = quantile(SD, 0.8),
           BotQtile = quantile(SD, 0.2))

# calculate high and low volatility months
Hi_Vol <- usdzarSD %>% filter(SD > TopQtile) %>% pull(YearMonth)
Low_Vol <- usdzarSD %>% filter(SD < BotQtile) %>% pull(YearMonth)

Perf_comparisons <- function(Idxs, YMs, Alias){
    # For stepping through uncomment:
    # YMs <- Hi_Vol
    Unconditional_SD <- 
        
        T40_Index_data %>% 
        
        group_by(Tickers) %>% 
        
        mutate(Full_SD = sd(Return) * sqrt(252)) %>% 
        
        filter(YearMonth %in% YMs) %>% 
        
        summarise(SD = sd(Return) * sqrt(252), 
                  across(.cols = starts_with("Full"), .fns = mean)) %>% 
        
        arrange(desc(SD)) %>% mutate(Period = Alias) %>% 
        
        group_by(Tickers) %>% 
        
        mutate(Ratio = SD / Full_SD)
    
    Unconditional_SD
    
}


perf_hi <- Perf_comparisons(Idxs = T40_Index_data , YMs = Hi_Vol, Alias = "High_Vol")

perf_lo <- Perf_comparisons(Idxs, YMs = Low_Vol, Alias = "Low_Vol")


```

The co-efficient of variation shows the extent of variability of data in a sample in relation to the mean of the population.

```{r message=FALSE, warning=FALSE}

perf_hi_low <- left_join(perf_hi %>% rename(High_Ratio = Ratio, SD_High = SD, FULL_SD_High = Full_SD) %>%
                             select(Tickers, High_Ratio, SD_High, FULL_SD_High),
                         
              perf_lo %>% rename(Low_Ratio = Ratio, SD_Low = SD, FULL_SD_LOW = Full_SD) %>%
                  select(Tickers, Low_Ratio, SD_Low, FULL_SD_LOW),
              by = "Tickers") %>% arrange(High_Ratio) %>% 
                    na.omit() 
    
#%>% filter(Tickers == c(""))

table <- knitr::kable(perf_hi_low , digits = 2, caption = "Summary of Mean Application Statistics per Category")
table

```

The 'Summary of Mean Application Statistics per Category' table provides standard deviation ratios that help determine with share prices experience changes in price during periods of low or high volatility. Therefore, one can gain insights as to which listed companies share prices are stable in periods of high volatility.


